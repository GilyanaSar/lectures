\section{Больше теории вероятностей}

Рассмотрим функцию $\xi$, определенную на вероятностном пространстве $\Omega$:$$\xi:\Omega\rightarrow \R$$
Математическим ожиданием $\xi$ называется такая величина:
$$\E\xi=\sum_{\omega\in\Omega}\xi(\omega) \P(\omega)$$ 

\begin{example} 
    Пусть вы - опытный продавец мороженного. Вам нужно заранее забронировать место в парке развелечений,
    поэтому вы хотите узнать, в какие дни стоит это делать. Вам известен прогноз погоды и то, сколько вы
    зарабатываете за сутки в определенную погоду. \par
    Вероятностное пространство событий:  $\Omega = \set{\text{жара, облачно, дождь, ураган}}$. \par
    $\xi(\text{жара}) = 100\$, \ \xi(\text{облачно}) = 50\$, \ \xi(\text{дождь}) = 10\$, \ \xi(\text{ураган}) = 1\$$. \par
    Аренда места обходится в $50\$$ за сутки. \par
    Пусть будет жарко с вероятностью $0.9$ и облачно с вероятностью $0.1$. Тогда:
    \[
        \E\xi = 0,9 \cdot 100\$ + 0,1 \cdot 50\$ - 50\$= 45\$
    \]
    Если все исходы равновероятны, то:
    \[
        \E\xi = 100 \cdot 0.25 + 50 \cdot 0.25 + 10 \cdot 0.25 + 1 \cdot 0.25 = 50\$
    \]
\end{example}

Введем определение индикаторной случайно величины. \par

\begin{definition}
    Индикаторная случайная величина $I_A$ события $A$ - величина, принимающая значение $1$, если событие $A$ произошло, и $0$ в обратном случае. \par
    \[
        I_A = 
        \left \lbrace
            \begin{array}{lc}
            1, & \P(A)\\
            0, & 1-\P(A)
            \end{array}
        \right.
    \]
\end{definition}

Тогда $\E I_A = \P(A)$. \par
Заметим, что для любого $r \subset R$ верно, что $\P(\xi \in r) = \sum_{\xi(\omega) \in r} \P(\omega)$. \par
Мы уже знакомы с понятием «независимые события». Теперь мы хотим понять, какие случайные величины являются независимыми. \par

\begin{definition}
    Случайные величины $\xi$ и $\psi$ независимы, если для любых $r_1 \in N(\xi)$ и $r_2 \in N(\psi)$, где $N(\ldots)$ - область значений величины, верно:
    \[
        \left \lbrace
        \begin{array}{l}
            A = \xi \in r_1\\
            B = \psi \in r_2
        \end{array}
        \right.
        \Rightarrow \P(A)\P(B) = \P(A \cap B)
    \]
\end{definition}

\begin{definition}
    Случайные величины $\xi$ и $\psi$ независимы, если для $\forall x,y \in \R$, верно:
    \[
        \left \lbrace
        \begin{array}{l}
            A = (\xi \equiv x) \\
            B = (\psi \equiv y)
        \end{array}
        \right.
        \Rightarrow \P(A)\P(B) = \P(A \cap B)
    \]
\end{definition}

Докажем, что эти определения эквивалентны.

\note{Я вообще не верю в то, что логика доказательства верна. Когда доказывается эквивалентность двух утверждений $A \Rightarrow B$ и $C \Rightarrow D$, то доказывается $A \Leftrightarrow C$.}

\begin{proof}
    Пусть $A_1 \ldots A_n$, где $n$ - число элементов в $r_1$. $A_i = (\xi = x_i), x_i \in r_1$. \par
    \begin{align*}
        & \P(A) = \P(A_1)+ \ldots +\P(A_n)
        & \P(B) = \P(B_1)+...+\P(B_m)
    \end{align*}

    \begin{align}
          & (\P(A_1) + \P(A_2) + \ldots + \P(A_n))(\P(B_1) + \P(B_2) + \ldots + \P(B_m)) = \\
        = & \sum_{i=1}^{n}\sum_{j=1}^{m}\P(A_i)\P(B_j) = \sum_{i=1}^{n}\sum_{j=1}^{m}\P(A_i \cap B_j) = \P(A \cap B)
    \end{align}
\end{proof}

Теперь рассмотрим самое главное свойство случайной величины. Мы будем использовать это свойство на протяжении всего курса алгоритмов. \par

\begin{theorem}
\[
    \E(\alpha\xi + \beta\psi) = \alpha \E\xi + \beta \E\psi
\]
\end{theorem}

\begin{proof}
    \begin{align*}
        \E(\alpha\xi + \beta\psi) = & \sum_{\omega \in \Omega}(\alpha\xi(\omega) + \beta\psi(\omega))\P(\omega) = \\
        = & \alpha \sum \xi(\omega)\P(\omega) + \beta \sum \psi(\omega)\P(\omega) = \\
        = & \alpha \E\xi + \beta \E\psi
    \end{align*}
\end{proof}

\begin{example}
    Найти математическое ожидание количества статичных точек($p_i = i$) в перестановке из $n$ элементов.
    \[
        \E\xi = \E(I_1 + ... I_n) = \sum_{i=1}^{n}\E I_i = \sum_{i=1}^{n}\P(I_i) = \sum_{i=1}^{n}\frac{1}{n}= 1
    \]
\end{example}

Мы теперь знаем, что $\E(\xi + \psi) = \E\xi + \E\psi$. Верно ли, что $\E(\xi \psi) = \E(\xi)\E(\psi)$? \par

\begin{theorem}
    Если $\xi$ и $\psi$ - независимые величины, то $\E(\xi \psi) = \E(\xi) \E(\psi)$.
\end{theorem}

\begin{proof}
    \begin{align*}
        \E\xi = & \sum_{\omega \in \Omega} \xi(\omega)\P(\omega)=\sum_{x\in N(\xi)}x\P(\xi = x) \\
        \E(\xi \psi) = & \sum z(\xi \psi = z) = \sum_{x \in N(\xi)}\sum_{y \in N(\psi)} \P((\xi = x)\cap (\psi = y)) = \\
        = & \sum_{x}\sum_{y}xy\P(\xi = x)\P(\psi = y) = \sum_{x}x\P(\xi = x) \sum_{y}y\P(\psi = y) = \E(\xi) \E(\psi)
    \end{align*}
\end{proof}

\begin{definition}
    Дисперсией величины $\xi$ называют такую величину $\D \xi$, что 
    \[
        \D\xi=\E(\xi - \E\xi)^2 = \E \xi^2 - (\E \xi)^2
    \]
\end{definition}

\begin{theorem}{\rm Неравенство Маркова (без док-ва)}
    \[
        \xi > 0, \ \P(\xi \geqslant{a}) \leqslant{\frac{\E\xi}{a}}
    \]
\end{theorem}

\begin{theorem}{\rm Неравенство Чебышёва (без дока-ва)}
    \[
        \P(|\xi - \E\xi| \geqslant{a}) \leqslant{\frac{\D\xi}{a^2}}
    \]
\end{theorem}

\begin{example}
    
    Найти максимальную возрастающую подпоследовательность в случайной перестановке $a_1...a_n$, то есть найти $j_1,,j_k$ такие, что $a_{j_1} < a_{j_2} < ... < a_{j_k}$.

    \begin{algorithm}
    \begin{algorithmic}
      \Let{$cur$}{$-1$}
      \For {$i:1 \to n$}
        \If{$a_i > cur$}
          \Let{$cur$}{$a_i$}
        \EndIf
      \EndFor
    \end{algorithmic}
    \end{algorithm}

\end{example}

\note{Что такое $A_k$?}

Пусть величина $\xi$ -- длина максимальной возрастающей последовательности. Тогда
\[
    \E\xi = \E(I_1 +...+ I_n) = \sum \E I_k = \sum \P(A_k) = \sum_{k=1}^{n} \frac{1}{k} = \log n.
\]


\begin{example}
    Дан взвешенный граф $G(V,E), w(e) \geq 0$. Найти максимальный разрез. (разрез в графе -- нетривиальное подмножество $A \subset V$ его вершин, величина разреза - сумма весов ребер, один конец которых лежит в $A$, другой - в $\overline{A}$). Тогда величина разреза это
    \[
        w(A)=\sum \limits_{u \in A \\ v \in \not{A}} s(uv),
    \]

    где $U_A$ - множество ребер, лежащих на разрезе, индуцированном множеством $A$. \par
    
    Решим задачу таким алгоримом: переберем все вершины, будем добавлять каждую из них в $A$ с
    вероятностью $\frac{1}{2}$. Докажем, что математическое ожидание величины $w(A)$ это хотя бы половина оптимального ответа. \par
    
    Вероятность того, что ребро лежит на оптимальном разрезе - $\frac{1}{2}$, потому либо оба конца лежат в $A$ или $\overline{A}$, либо один из концов лежит в $A$, другой - в $\overline{A}$. \par
    \[
        \E\xi = \sum \E I(e \in U_A)w(e) = \sum w(e)\P(e \in U_A) = \sum \frac{1}{2} w(e) = \frac{1}{2} \sum w(e)
    \]
\end{example}

\begin{example}
    Приведем пример алгоритма построения случайного дерева. Пусть 1-я вершина - это корень. \par

\rm{
  \begin{algorithm}
    \begin{algorithmic}
      \For {$i:2 \to n$}
        \If{$a_i > cur$}
          \Let{$\text{parent}(i)$}{$\text{rand}(1, i - 1)$}
        \EndIf
      \EndFor
    \end{algorithmic}
  \end{algorithm}
}
    Докажем, что $\forall u,v \ \E g(v,u)=O(\log n)$.
    \begin{align*}
        & \E g(v,u) \leqslant \E(d(v) + d(u) = \E d(v) + \E d(u) \\
        & \E d(i)=\log i (i \geqslant{2},d(1)=0) \\
        & \E d(i)=1+\sum_{j=1..i-1} \E d(i) \frac{1}{i-1} \\
        & \exists c: Ed(i) \leqslant{c \log n} \\
        & \E d(i) \leqslant{ 1+ \frac{1}{i-1} \sum c \log i} = \\
        & = 1 + \frac{\sum_{j=1}^{\frac{i}{2}} c \log j}{i-1} + \frac{\sum_{j=\frac{i}{2}+1}^{i} c \log j}{i-1} \\
        & \E d(i) \leqslant{1+\frac{1}{i-1} \sum c\log (i-1) + \frac{1}{i-1} \sum c\log i} = 1 + \frac{1}{i-1} \sum c\log i
    \end{align*}
\end{example}