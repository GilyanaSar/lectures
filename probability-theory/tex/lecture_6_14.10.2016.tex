\section{Лекция от 14.10.2016}

\subsection{Локальная лемма Ловаса: несимметричный и симметричный случаи}
\textbf{Предупреждение}: в данном случае мы решили несколько изменить доказательство, отойдя от сигма-алгебр к графам зависимости. Идейно эти принципы ничем не отличаются, но на коллоквиуме лучше рассказывать через алгебры событий.

Введём понятие \emph{графа зависимости}.

\begin{definition}
    \emph{Графом зависимости} \(D = (V, E)\) для событий \(A_1, A_2, \dots, A_n\) называют ориентированный граф такой, что \(V = \{1, 2, \dots, n\}\), а для любого \(1 \leq i \leq n\) событие \(A_i\) независимо с событиями \(A_j\), если \((i, j) \notin E\).
\end{definition}

\begin{theorem}[Несимметричный случай локальной леммы Ловаса]
    Пусть \(A_1, \dots, A_n\) --- события на произвольном вероятностном пространстве \((\Omega, \Pr)\). Предположим, что
    \begin{enumerate}
        \item \(D = (V, E) \) является орграфом зависимости для определенных выше событий;
        \item Существуют действительные числа \(x_1, \ldots x_n \in [0, 1) \) такие, 
        что для любого \(1 \leq i \leq n\) выполняется \[\Pr{A_i} \leq x_i \prod\limits_{(i, j) \in E} (1 - x_j).\]
    \end{enumerate}
    
    Тогда вероятность того, что ни одно из событий \(A_1, \dots, A_n\) не произойдёт, можно ограничить снизу положительным числом: \[\Pr{\bigcap\limits_{i = 1}^{n}\overline{A_i}} \geq \prod\limits_{i = 1}^{n}(1 - x_i) > 0.\]
\end{theorem}

Перед тем, как приступать к доказательству этой леммы, докажем следующую формулу:
\begin{theorem}[Формула умножения вероятностей]
    Пусть \(B_1, B_2, \dots, B_n\)~--- события на вероятностном пространстве \((\Omega, \Pr)\). Тогда верно следующее:
    \[\Pr{B_1 \cap \dots \cap B_m} = \Pr{B_1 \given B_2 \cap \dots \cap B_m}\Pr{B_2 \given B_3 \cap \dots \cap B_m} \dotsm \Pr{B_{m-1} \given B_m}\Pr{B_m}.\]
\end{theorem}
\begin{proof}
    Будем последовательно применять определение условной вероятности:
    \[\begin{aligned}
    \Pr{B_1 \cap \dots \cap B_m} &= \Pr{B_1 \given B_2 \cap \dots \cap B_m}\Pr{B_2 \cap \dots \cap B_m} \\
    \Pr{B_2 \cap \dots \cap B_m} &= \Pr{B_2 \given B_3 \cap \dots \cap B_m}\Pr{B_3 \cap \dots \cap B_m} \\
    &\dots \\
    \Pr{B_{m - 1} \cap B_m}&= \Pr{B_{m-1} \given B_m}\Pr{B_m}.
    \end{aligned}\]
    
    Постепенными подстановками получим желаемое.
\end{proof}

Теперь перейдём к доказательству леммы.
\begin{proof}
    Докажем следующее утверждение: для любого \(1 \leq i \leq n\) и любого набора чисел \(T \subset \{1, 2, \dots, n\} \setminus \{i\}\) справедливо неравенство \[\Pr{A_i \given \bigcap_{j \in T} \overline{A_j}} \leq x_i.\]
    
    В основе доказательства лежит индукция по размеру множества \(T\). Базовый случай~--- \(T = \emptyset\). Тогда из условия следует, что \(\Pr{A_i} \leq x_i\).
    
    Теперь предположим, что утверждение верно для любых множеств размера \(t' < t\). Докажем, что из этого следует, что оно выполняется и для множеств размера \(t\). Для этого зафиксируем произвольное множество \(T\) размера \(t\) и построим два множества: \(T_1 = \{j \in T \mid (i, j) \in E\}\) и \(T_2 = T \setminus T_1\). Тогда
    
    \[\begin{aligned}
    \Pr{A_i \given \bigcap_{j \in T} \overline{A_j}} &= \Pr{A_i \given \left(\bigcap_{j \in T_1} \overline{A_j}\right) \cap \left(\bigcap_{j \in T_2} \overline{A_j}\right)}
    = \frac{\Pr{A_i \cap \left(\bigcap\limits_{j \in T_1} \overline{A_j}\right) \cap \left(\bigcap\limits_{j \in T_2} \overline{A_j}\right)}}{\Pr{\left(\bigcap\limits_{j \in T_1} \overline{A_j}\right) \cap \left(\bigcap\limits_{j \in T_2} \overline{A_j}\right)}} \\
    &= \frac{\Pr{A_i \cap \left(\bigcap\limits_{j \in T_1} \overline{A_j}\right) \given \left(\bigcap\limits_{j \in T_2} \overline{A_j}\right)}}{\Pr{\left(\bigcap\limits_{j \in T_1} \overline{A_j}\right) \given \left(\bigcap\limits_{j \in T_2} \overline{A_j}\right)}} \leq \frac{\Pr{A_i \given \left(\bigcap\limits_{j \in T_2} \overline{A_j}\right)}}{\Pr{\left(\bigcap\limits_{j \in T_1} \overline{A_j}\right) \given \left(\bigcap\limits_{j \in T_2} \overline{A_j}\right)}}.
    \end{aligned}\]
    
    Из определения \(T_2\) следует, что \(A_i\) не зависит от событий \(\{A_j \mid j \in T_2\}\). Но тогда
    \[\Pr{A_i \given \left(\bigcap\limits_{j \in T_2} \overline{A_j}\right)} = \Pr{A_i} \leq x_i \prod\limits_{(i, j) \in E} (1 - x_j).\]
    
    Теперь рассмотрим числитель. Положим, что \(T_1 = \{j_{1}, j_{2}, \dots, j_{m}\}\). Если вышло так, что \(m = 0\), то знаменатель равен 1 и нужное нам неравенство выполняется. Иначе же воспользуемся формулой произведения вероятностей и предположением индукции:
    \[\begin{aligned}
    \Pr{\left(\bigcap\limits_{j \in T_1} \overline{A_j}\right) \given \left(\bigcap\limits_{j \in T_2} \overline{A_j}\right)} &= \Pr{A_{j_1} \given \left(\bigcap\limits_{s = 2}^{m}\overline{A_{j_s}}\right) \cap \left(\bigcap\limits_{j \in T_2} \overline{A_j}\right)}\Pr{\left(\bigcap\limits_{s = 2}^{m}\overline{A_{j_s}}\right) \cap \left(\bigcap\limits_{j \in T_2} \overline{A_j}\right)} \\
    &\geq (1 - x_{j_1})\Pr{\left(\bigcap\limits_{s = 2}^{m}\overline{A_{j_s}}\right) \cap \left(\bigcap\limits_{j \in T_2} \overline{A_j}\right)} \\
    &\geq (1 - x_{j_1})(1 - x_{j_2})\Pr{\left(\bigcap\limits_{s = 3}^{m}\overline{A_{j_s}}\right) \cap \left(\bigcap\limits_{j \in T_2} \overline{A_j}\right)} \\
    &\geq \dots \\
    &\geq \prod_{s = 1}^{m} (1 - x_{j_s}) \geq \prod\limits_{(i, j) \in E} (1 - x_j).
    \end{aligned}\]
    
    Тогда
    \[\Pr{A_i \given \bigcap_{j \in T} \overline{A_j}} \leq \frac{x_i \prod\limits_{(i, j) \in E} (1 - x_j)}{\prod\limits_{(i, j) \in E} (1 - x_j)} = x_i \implies \Pr{\overline{A_i} \given \bigcap_{j \in T} \overline{A_j}} \geq 1 - x_i.\]
    
    Осталось лишь применить полученный результат к вероятности того, что ни одно из событий \(A_1, A_2, \dots, A_n\) не выполнится:
    
    \[\Pr{\bigcap\limits_{i = 1}^{n} \overline{A_i}} = \Pr{\overline{A_1} \given \bigcap\limits_{i = 2}^{n} \overline{A_i}}\Pr{\overline{A_2} \given \bigcap\limits_{i = 3}^{n} \overline{A_i}}\dotsc\Pr{\overline{A_{n - 1}} \given \overline{A_n}}\Pr{\overline{A_n}} \geq \prod\limits_{i = 1}^{n}(1 - x_i) > 0. \qedhere\]
\end{proof}
\begin{exercise}
    Сформулируйте и докажите несимметрический случай локальной леммы Ловаса, используя алгебры событий вместо графа зависимости.
\end{exercise}

Однако на практике гораздо чаще используется более слабая версия леммы.

\begin{theorem}[Симметричный случай локальной леммы Ловаса]
    Пусть \(A_1, A_2, \dots, A_n\)~--- события в произвольном вероятностном пространстве \((\Omega, \Pr)\). Если выполнены следующие условия:
    \begin{enumerate}
        \item существует \(p \in [0, 1)\) такое, что \(\Pr{A_i} \leq p\) для любого \(1 \leq i \leq n\);
        \item каждое событие \(A_i\) взаимно независимо со всеми событиями за исключением не более, чем \(d\) событий;
        \item \(ep(d + 1) \leq 1\),
    \end{enumerate}
    то \[\Pr{\bigcap\limits_{i = 1}^{n}\overline{A_i}} > 0.\]
\end{theorem}
\begin{proof}
    Для начала рассмотрим экстремальный случай~--- \(d = 0\). В таком случае все события независимы и утверждение теоремы выполнено.
    
    Теперь перейдём к общему случаю. Рассмотрим граф зависимости \(D = (V, E)\) для событий \(A_1, A_2, \dots, A_n\), в котором для любого \(i \in \{1, 2, \dots, n\}\) выполнено \(\left|\{j \mid (i, j) \in E\}\right| \leq d\). Далее, для всех \(i\) положим \(x_i = \frac{1}{d + 1}\). Покажем, что в этом случае выполняется требование несимметричного случая локальной леммы Ловаса:
    
    \[x_i\prod\limits_{(i, j) \in E} (1 - x_j) \geq \frac{1}{d + 1}\left(1 - \frac{1}{d + 1}\right)^{d} \geq \frac{1}{e(d + 1)} \geq p \geq \Pr{A_i}.\]
    
    Как видно, оно выполняется. Тогда по локальной лемме получаем, что
    \[\Pr{\bigcap\limits_{i = 1}^{n}\overline{A_i}} > 0. \qedhere\]
\end{proof}

\subsection{Задача \textsf{k-SAT}. Теорема Мозеса-Тардоша}
Сейчас мы обсудим так называемую задачу \textsf{k-SAT} и её методы решения, основанные на вероятностном подходе. Но начнём мы с формулировки задачи.

\begin{definition}
    Пусть есть некоторая булева формула \(f(x_1, x_2, \dots, x_n)\). Тогда \emph{\(k\)-КНФ} называют представление этой формулы в виде
    \[f(x_1, x_2, \dots, x_n) = \bigwedge\limits_{i = 1}^{m} \left(l_{i1} \lor l_{i2} \lor \dots \lor l_{ik}\right),\]
    
    где \(l_{ij} \in \{x_1, \dots, x_n\} \cup \{\overline{x_1}, \overline{x_2}, \dots, \overline{x_n}\}\), причём в дизъюнкте все переменные разные (то есть нет повторов).
\end{definition}

\begin{problem}[\textsf{k-SAT}]
    Пусть есть некоторая булева функция \(f(x_1, x_2, \dots, x_n)\), представленная в виде
    \(k\)-КНФ. Выполнима ли функция \(f\), то есть существует ли такой набор переменных \(v_1, v_2, \dots, v_n\), что \(f(v_1, v_2, \dots, v_n) = 1\)?
\end{problem}

Докажем следующее утверждение:
\begin{theorem}
    Если \(k\)-КНФ для функции \(f(x_1, x_2, \dots, x_n)\) содержит меньше, чем \(2^k\) дизъюнктов, то \(f\) выполнима.
\end{theorem}
\begin{proof}
    Рассмотрим ситуацию, когда переменным равновероятно и независимо присваивается либо 0, либо 1. Тогда \(\Pr{x_i = 0} = \Pr{x_i = 1} = \frac{1}{2}\) для \(i \in \{1, 2, \dots, n\}\). 
    
    Чему равна вероятность того, что дизъюнкт \(l_{i1} \lor l_{i2} \lor \dots \lor l_{ik}\) будет равен 0? Она равна
    \[\Pr{l_{i1} = 0, l_{i2} = 0, \dots, l_{ik} = 0} = \Pr{l_{i1} = 0}\Pr{l_{i2} = 0} \dotsc\Pr{l_{ik} = 0} = \frac{1}{2^k}.\]
    
    Теперь посмотрим вероятность того, что вся КНФ будет равна 0:
    \[\begin{aligned}
    \Pr{f(x_1, x_2, \dots, x_n) = 0} &= \Pr{\bigcup_{i = 1}^{m} \{l_{i1} \lor l_{i2} \lor \dots \lor l_{ik} = 0\}} \\
    &\leq \sum_{i = 1}^{m}\Pr{l_{i1} \lor l_{i2} \lor \dots \lor l_{ik} = 0} = \frac{m}{2^k} < 1.
    \end{aligned}\]
    
    Отсюда получаем, что вероятность того, что КНФ будет равна 1, больше нуля. А это и означает выполнимость \(f\).
\end{proof}

Теперь посмотрим на следующий вопрос. Пусть каждая переменная (или её отрицание) лежит не более, чем в \(d\) скобках. При каком \(d\) мы можем гарантировать выполнимость? Ответ на этот вопрос даёт следующая теорема:

\begin{theorem}
    Пусть булева формула \(f(x_1, x_2, \dots, x_n)\) записана в виде \(k\)-КНФ. Также пусть любая переменная или ее отрицание входят в не более, чем \(\frac{2^{k-1}}{ek}\) дизъюнкций. Тогда \(f\) выполнима.
\end{theorem}

\begin{proof}
    Так же, как и в прошлой теореме, рассмотрим ситуацию, когда переменным равновероятно и независимо присваивается либо 0, либо 1. Тогда \(\Pr{x_i = 0} = \Pr{x_i = 1} = \frac{1}{2}\) для \(i \in \{1, 2, \dots, n\}\). 
    
    Рассмотрим событие \(A_i = \{i\)-й дизъюнкт равен \(0\}\). Исходя из того, что каждый дизъюнкт содержит \(k\) литералов, а также того, что все значения переменных равновероятны, мы можем оценить вероятность каждого события следующим образом:
    \[\Pr{A_i} = \begin{cases}
    0,&\text{в дизъюнкте одновременно есть }y\text{ и }\overline{y}, \\
    \frac{1}{2^k},&\text{иначе}.
    \end{cases}\]
    
    Отсюда видно, что \(\Pr{A_i} \leq \frac{1}{2^k}\) для всех \(i \in \{1, 2, \dots, m\}\).
    
    Теперь посмотрим на количество событий, зависимых с \(A_i\). Для этого рассмотрим зависимость на каком-либо примере. Пусть \(i\)-й дизъюнкт равен \(x_1 \lor \overline{x_2} \lor \overline{x_3} \lor \dots \lor \overline{x_k}\). Тогда \(A_i\) будет независимо с теми событиями, соответствующие дизъюнкты которых не содержат элементов из \(\{x_1, x_2, \dots, x_k\} \cup \{\overline{x_1}, \overline{x_2}, \dots, \overline{x_k}\}\).
    
    Тогда количество событий, зависимых с \(A_i\) равно количеству дизъюнктов, совпадающих хотя бы по одной переменной (с точностью до отрицания). Обозначим его за \(S_i\). Попробуем ограничить его сверху. Выберем произвольную позицию, которая будет общей. Так как переменная в этой позиции входит в не более, чем \(\frac{2^{k-1}}{ek}\) дизъюнкций и переменные должны совпадать с точностью до отрицания, то есть не более, чем \(2k\frac{2^{k - 1}}{ek} = \frac{2^k}{e}\). Однако мы посчитали и сам дизъюнкт. Поэтому
    \[|S_i| \leq \frac{2^{k}}{e} - 1 = d.\]
    
    Теперь покажем, что выполняется и последнее условие симметричного случая локальной леммы Ловаса: \(ep(d + 1) \leq \frac{e}{2^k}\frac{2^k}{e} = 1.\)
    
    Отсюда по симметричной локальной лемме Ловаса получаем, что \(\Pr{\bigcap_{i = 1}^{n} \overline{A_i}} > 0\). Это означает, что вероятность того, что при заданном наборе переменных функция равна 1 больше нуля, что доказывает выполнимость \(f\).
\end{proof}

Из общего случая локальной леммы можно вывести следующую оценку для задачи выше. 
\[\Pr{\bigcap\limits_{i = 1}^{m}\overline{A_i}} \geq \prod\limits_{i = 1}^{m}(1 - x_i) = \left(\frac{d}{d+1}\right)^{m} = \left(1 - \frac{e}{2^k} \right)^{m} = q.\]

Теперь можно задаться вполне резонным вопросом: а как все же искать такое выполняющее означивание? Рассмотрим два подхода к решению этой проблемы.
\begin{enumerate}
    \item \emph{Наивный подход}: подставляем случайные значения \((x_1, x_2, \dots, x_n )\). Если \(f = 0\), тогда независимо подставим новые значения \((y_1, y_2, \dots, y_n)\). Так будем повторять до победного конца.
    
    Пусть \(X\)~--- число подстановок новых пар значений переменных (\emph{resampling steps}). Легко понять, что \(X \sim \mathrm{Geom}(q)\) и\footnote{За доказательством этого обращайтесь ко второй задаче 4-го семинара.}
    \[\E{X} = \sum_{k = 0}^{\infty} kq(1 - q)^{k} = \frac{1}{q} = \mathcal{O}\left(\left(\frac{2^k}{2^k - e}\right)^{m}\right).\]
    
    \item \emph{Продвинутый подход}: подставляем случайные значения \((x_1, x_2, \dots, x_n )\). Если \(f = 0\), то смотрим, какие именно дизъюнкты равны 0. Если \(l_{i1} \lor l_{i2} \lor \dots \lor l_{ik} = 0\), то меняем значения переменных \(l_{i1}, l_{i2}, \dots, l_{ik}\) на случайные независимым образом. Данная операция и будет служить \emph{resampling step}. Продолжаем это до тех пор, пока не получим \(f = 1\).
\end{enumerate}

Алгоритм в продвинутом подходе крепко связан с локальной леммой, ибо это есть ни что иное, как алгоритмическая версия локальной леммы Ловаса. Данный алгоритм называется \emph{алгоритмом Мозера-Тардоша} и для него верна следующая теорема:\footnote{-- А мы не доказываем эту теорему? \ -- Нет. Это тяжело. (Д.А. Шабанов)}

\begin{theorem}
    Пусть \(Y_i\)~--- количество resampling steps для события \(A_i  = \{i\)-й дизъюнкт равен \(0\}\) В условиях задачи \textsf{k-SAT} для любых \(1 \leq  i \leq m \), где \(m\)~--- это количество дизъюнктов в формуле, \(\E[Y_i] \leq \frac{1}{d} = \frac{2^k}{e} - 1\).
    
    Пусть \(Y = \sum\limits_{i = 1}^{m}Y_i \)~--- общее число resampling steps. Тогда \(\E[Y] \leq \frac{m}{d} = \mathcal{O}(m). \)
\end{theorem}

\subsection{Геометрические вероятности. Задача о встрече}

\begin{definition}
    Пусть вероятностное пространство \(\Omega \in \R^n \) таково, что у \(\Omega\) определен \(n\)-мерный объем, \(0 < \mu(\Omega) < \infty\). Тогда в геометрических вероятностях для любого \(A \in \Omega\), у которого тоже определен объем \(\mu(A)\), полагаем вероятность \(\Pr(A) = \frac{\mu(A)}{\mu(\Omega)}\). Это естественное обобщение классической модели на непрерывный случай. 
\end{definition}
\begin{remark}
    Естественно оно по той простой причине, что аналогично дискретно случаю. Да и вообще вполне логично.
\end{remark}

Для чего вводится такое определение? Бывают ситуации, когда конечным числом элементарных исходов обойтись нельзя. Ярким примером этому служит классическая \emph{задача о встрече}

\begin{problem}[Задача о встрече]
    Два друга договорились встретиться на автобусной остановке между 9-ю и 10-ю часами утра. Они договорились, что встретятся в течение этого часа, но не условились о конкретном времени встречи. При этом, если, прождав 15 минут с момента своего прихода, один из друзей не видит второго, он уходит, а значит встреча не состоялась. Вопрос: с какой вероятностью друзья встретятся?
\end{problem}
\begin{proof}[Решение]
    Вообще, то, что конечности вероятностного пространства не хватит, понятно сразу, ведь моменты времени, в которые люди приходят на остановку, конечно же, не дискретны. 
    Отложим на оси \(x\) и на оси \(y\) по отрезку длины 1, которые будут символизировать час между 9-ю и 10-ю часами, в течение которого на остановку приходит первый и второй человек соответственно. Итак, вероятностное пространство \(\Omega = [9, 10] \times [9, 10].\) 
    
    Событию \(A\), что встреча произойдет, удовлетворяют все точки получившегося квадрата, координаты которых отличатся не более, чем на четверть. То есть, моменты времени прихода друзей отличаются не более, чем на 15 минут. \(A = \{\)встреча произойдет\(\} = \{(x, y) \mid |x - y| \leq \frac{1}{4}\} \). Изобразим это:
    
    \begin{center}
        \begin{tikzpicture}
        \fill [white] (0, 0) -- (0, 4) -- (4, 4) -- (4, 0) -- cycle;
        \fill [pattern = crosshatch] (1, 0) -- (0, 1) -- (3, 4) -- (4, 3) -- cycle;
        \draw [thick, ->] (0, -0.5) -- (0, 4.5) node [anchor = east] {\(y\)};
        \draw [thick, ->] (-0.5, 0) -- (4.5, 0) node [anchor = north] {\(x\)};
        \draw (0, 0) node [anchor = north east] {\(9\)};
        \draw (0, 4) node [anchor = east] {\(10\)};
        \draw (4, 0) node [anchor = north] {\(10\)};
        \draw (0, 0) -- (0, 4) -- (4, 4) -- (4, 0) -- cycle;
        \draw (1, 0) -- (0, 1) -- (3, 4) -- (4, 3) -- cycle;
        \end{tikzpicture}
    \end{center}
    
    Довольно интуитивно хочется посчитать вероятность $ \Pr(A) $ как отношение площадей закрашенной области (всех удовлетворяющих исходов) ко всей площади квадрата (множество всех исходов). Это и будет геометрическая вероятность. 
    \[
    \Pr(A) = 1 - \left(\frac{3}{4} \right)^2 = \frac{7}{16}. \qedhere
    \]
    
    
\end{proof}